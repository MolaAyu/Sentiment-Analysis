{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1\n",
    "\"\"\"This file shows a couple of implementations of the perceptron learning\n",
    "algorithm. It is based on the code from Lecture 3, but using the slightly\n",
    "more compact perceptron formulation that we saw in Lecture 6.\n",
    "\n",
    "There are two versions: Perceptron, which uses normal NumPy vectors and\n",
    "matrices, and SparsePerceptron, which uses sparse vectors and matrices.\n",
    "The latter may be faster when we have high-dimensional feature representations\n",
    "with a lot of zeros, such as when we are using a \"bag of words\" representation\n",
    "of documents.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "class LinearClassifier(BaseEstimator):\n",
    "    \"\"\"\n",
    "    General class for binary linear classifiers. Implements the predict\n",
    "    function, which is the same for all binary linear classifiers. There are\n",
    "    also two utility functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Computes the decision function for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "        return X.dot(self.w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the outputs for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "\n",
    "        # First compute the output scores\n",
    "        scores = self.decision_function(X)\n",
    "\n",
    "        # Select the positive or negative class label, depending on whether\n",
    "        # the score was positive or negative.\n",
    "        out = np.select([scores >= 0.0, scores < 0.0],\n",
    "                        [self.positive_class,\n",
    "                         self.negative_class])\n",
    "        return out\n",
    "\n",
    "    def find_classes(self, Y):\n",
    "        \"\"\"\n",
    "        Finds the set of output classes in the output part Y of the training set.\n",
    "        If there are exactly two classes, one of them is associated to positive\n",
    "        classifier scores, the other one to negative scores. If the number of\n",
    "        classes is not 2, an error is raised.\n",
    "        \"\"\"\n",
    "        classes = sorted(set(Y))\n",
    "        if len(classes) != 2:\n",
    "            raise Exception(\"this does not seem to be a 2-class problem\")\n",
    "        self.positive_class = classes[1]\n",
    "        self.negative_class = classes[0]\n",
    "\n",
    "    def encode_outputs(self, Y):\n",
    "        \"\"\"\n",
    "        A helper function that converts all outputs to +1 or -1.\n",
    "        \"\"\"\n",
    "        return np.array([1 if y == self.positive_class else -1 for y in Y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using hinge loss function algorithm\n",
    "class Pegasos_SVC(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=1000):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "        # Pegasos algorithm 1 using hinge loss objective function for the linear SVC classfier. The mehtod defiens the \n",
    "        # input and output of the loss function. Also, it defiens the regulaization parameter as r. \n",
    "        t=0\n",
    "        self.r=  1/(n_features)# Regularization(λ)\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in list(zip(X, Ye)):\n",
    "                t=t+1\n",
    "                eta = 1/(t*self.r) # learning ratein gradient descent(η)\n",
    "                # Compute the output score for this instance.\n",
    "                score = x.dot(self.w)\n",
    "                # If there was an error, update the weights.\n",
    "                if y*score <1:\n",
    "                    self.w= (1- eta*self.r)*(self.w) + (eta*y)*x\n",
    "                \n",
    "                else:\n",
    "                    self.w= (1-eta*self.r)*(self.w)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using log loss function algorithm\n",
    "class Pegasos_LR(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=1000):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "        t=0\n",
    "        self.r= 1/(n_features)# Regularization(λ)\n",
    "        # Pegasos algorithm 2 using log loss objective function:\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in list(zip(X, Ye)):\n",
    "                t=t+1\n",
    "                gl= 1/(t*self.r) # learning ratein gradient descent(η)\n",
    "                # Compute the output score for this instance.\n",
    "                score = x.dot(self.w)\n",
    "                # If there was an error, update the weights.\n",
    "                grad_log_loss= (y/(1+np.exp(y*score)))*x\n",
    "                \n",
    "                self.w= (1-gl*self.r)*(self.w) + gl*grad_log_loss\n",
    "                \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 126.33 sec.\n",
      "Accuracy: 0.8267.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from aml_perceptron import Perceptron, SparsePerceptron\n",
    "\n",
    "# This function reads the corpus, returns a list of documents, and a list\n",
    "# of their corresponding polarity labels. \n",
    "def read_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            _, y, _, x = line.split(maxsplit=3) # For binary classification\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Read all the documents.\n",
    "    X, Y = read_data(r'C:\\Users\\Amerm\\Desktop\\pa2b\\data\\all_sentiment_shuffled.txt')\n",
    "    # Split into training and test parts.\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,random_state=0)\n",
    "\n",
    "    # Set up the preprocessing steps and the classifier.\n",
    "    pipeline = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "\n",
    "        # NB that this is our Perceptron, not sklearn.linear_model.Perceptron\n",
    "        Pegasos_SVC() \n",
    "       # Pegasos_LR()\n",
    "    )\n",
    "\n",
    "    # Train the classifier.\n",
    "    t0 = time.time()\n",
    "    pipeline.fit(Xtrain, Ytrain)\n",
    "    t1 = time.time()\n",
    "    print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "    # Evaluate on the test set.\n",
    "    Yguess = pipeline.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy results for the linear classifier SVC was 0.83 and for the logistic classifeir was 0.8 so both of them are \n",
    "fullfiling the goal from implemnting these classifier after transling the pseudo code to python program. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "import random\n",
    "import scipy as sp\n",
    "\n",
    "class LinearClassifier(BaseEstimator):\n",
    "    \"\"\"\n",
    "    General class for binary linear classifiers. Implements the predict\n",
    "    function, which is the same for all binary linear classifiers. There are\n",
    "    also two utility functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Computes the decision function for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "        return X.dot(self.w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the outputs for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "\n",
    "        # First compute the output scores\n",
    "        scores = self.decision_function(X)\n",
    "        A = []\n",
    "        for row in scores:\n",
    "            A.append(np.argmax(row))\n",
    "\n",
    "        # Select the positive or negative class label, depending on whether\n",
    "        # the score was positive or negative.\n",
    "        out = self.decode_multi_outputs(A)\n",
    "        return out\n",
    "\n",
    "    def find_classes(self, Y):\n",
    "        \"\"\"\n",
    "        Finds the set of output classes in the output part Y of the training set.\n",
    "        If there are exactly two classes, one of them is associated to positive\n",
    "        classifier scores, the other one to negative scores. If the number of\n",
    "        classes is not 2, an error is raised.\n",
    "        \"\"\"\n",
    "        classes = sorted(set(Y))\n",
    "        if len(classes) != 2:\n",
    "            raise Exception(\"this does not seem to be a 2-class problem\")\n",
    "        self.positive_class = classes[1]\n",
    "        self.negative_class = classes[0]\n",
    "\n",
    "    def encode_outputs(self, Y):\n",
    "        \"\"\"\n",
    "        A helper function that converts all outputs to +1 or -1.\n",
    "        \"\"\"\n",
    "        return np.array([1 if y == self.positive_class else -1 for y in Y])\n",
    "\n",
    "    def find_number_of_classes(self, Y):\n",
    "        i = 0\n",
    "        classes = sorted(set(Y))\n",
    "        for c in classes:\n",
    "            print(c, \"has number\", i)\n",
    "            i += 1\n",
    "        return len(classes)\n",
    "\n",
    "    def encode_multi_outputs(self, Y):\n",
    "        switcher = {\n",
    "            \"books\": 0,\n",
    "            \"camera\": 1,\n",
    "            \"dvd\": 2,\n",
    "            \"health\": 3,\n",
    "            \"music\": 4,\n",
    "            \"software\": 5\n",
    "        }\n",
    "        Ye = list(map(switcher.get, Y))\n",
    "        return Ye\n",
    "\n",
    "    def decode_multi_outputs(self, Y):\n",
    "        switcher = {\n",
    "            0: \"books\",\n",
    "            1: \"camera\",\n",
    "            2: \"dvd\",\n",
    "            3: \"health\",\n",
    "            4: \"music\",\n",
    "            5: \"software\"\n",
    "        }\n",
    "        Yd = list(map(switcher.get, Y))\n",
    "        return Yd\n",
    "##### The following part is for the optional task.\n",
    "\n",
    "### Sparse and dense vectors don't collaborate very well in NumPy/SciPy.\n",
    "### Here are two utility functions that help us carry out some vector\n",
    "### operations that we'll need.\n",
    "\n",
    "def add_sparse_to_dense(x, w, factor):\n",
    "    \"\"\"\n",
    "    Adds a sparse vector x, scaled by some factor, to a dense vector.\n",
    "    This can be seen as the equivalent of w += factor * x when x is a dense\n",
    "    vector.\n",
    "    \"\"\"\n",
    "    w[x.indices] += factor * x.data\n",
    "\n",
    "def sparse_dense_dot(x, w):\n",
    "    \"\"\"\n",
    "    Computes the dot product between a sparse vector x and a dense vector w.\n",
    "    \"\"\"\n",
    "    return np.dot(w[x.indices], x.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClassSVM(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=100000):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the pegasos learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        #self.find_classes(Y)\n",
    "        classNumber = self.find_number_of_classes(Y)\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        #Ye = self.encode_outputs(Y)\n",
    "        Ye = self.encode_multi_outputs(Y)\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros((n_features, classNumber))\n",
    "        \n",
    "        self.r=1/(n_features)\n",
    "        \n",
    "        XY = list(zip(X, Ye))\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            x, y = random.choice(XY)\n",
    "            t = i + 1\n",
    "            eta = 1/(t*self.r)\n",
    "            # Compute the output score for this instance.\n",
    "            scoreYi = x.dot(self.w[:, y])\n",
    "            A = []\n",
    "            for j in range(classNumber):\n",
    "                scoreY = x.dot(self.w[:, j])\n",
    "                if y == j:\n",
    "                    A.append(0 - scoreYi + scoreY)\n",
    "                else:\n",
    "                    A.append(1 - scoreYi + scoreY)\n",
    "            yHat = np.argmax(A)\n",
    "            m1 = np.zeros((n_features, classNumber))\n",
    "            m2 = np.zeros((n_features, classNumber))\n",
    "            m1[:, yHat] = x\n",
    "            m2[:, y] = x\n",
    "            self.w = (1 - eta * self.r) * self.w - eta * (m1 - m2)\n",
    "\n",
    "\n",
    "\n",
    "class MultiClassLR(LinearClassifier):\n",
    "    \"\"\"\n",
    "    A straightforward implementation of the perceptron learning algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter=100000):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the pegasos learning algorithm.\n",
    "        \"\"\"\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        #self.find_classes(Y)\n",
    "        classNumber = self.find_number_of_classes(Y)\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        #Ye = self.encode_outputs(Y)\n",
    "        Ye = self.encode_multi_outputs(Y)\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros((n_features, classNumber))\n",
    "        self.r= 1/(n_features)\n",
    "        t=0\n",
    "        XY = list(zip(X, Ye))\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            x, y = random.choice(XY)\n",
    "            t = t + 1\n",
    "            eta = 1/(t*self.r)\n",
    "            # Compute the output score for this instance.\n",
    "            scores = x.dot(self.w)\n",
    "            grad_log_loss = np.zeros((n_features, classNumber))\n",
    "            p = sp.special.softmax(scores)\n",
    "            p12 = np.zeros((n_features, classNumber))\n",
    "            p12[:, y] = x\n",
    "            for r in range(classNumber):\n",
    "                p34 = np.zeros((n_features, classNumber))\n",
    "                p34[:, r] = x\n",
    "                grad_log_loss += p * p34 - p12\n",
    "                \n",
    "            self.w = (1 - eta * self.r) * self.w - eta * grad_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books has number 0\n",
      "camera has number 1\n",
      "dvd has number 2\n",
      "health has number 3\n",
      "music has number 4\n",
      "software has number 5\n",
      "Training time: 12.28 sec.\n",
      "Accuracy: 0.9287.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from aml_perceptron import Perceptron, SparsePerceptron\n",
    "\n",
    "# This function reads the corpus, returns a list of documents, and a list\n",
    "# of their corresponding polarity labels. \n",
    "def read_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            #_, y, _, x = line.split(maxsplit=3)\n",
    "            y, _, _, x = line.split(maxsplit=3) # For multi-class classification\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Read all the documents.\n",
    "    X, Y = read_data(r'C:\\Users\\Amerm\\Desktop\\pa2b\\data\\all_sentiment_shuffled.txt')\n",
    "    # Split into training and test parts.\n",
    "    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2,random_state=0)\n",
    "\n",
    "    # Set up the preprocessing steps and the classifier.\n",
    "    pipeline = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "        #MultiClassLR()\n",
    "        # NB that this is our Perceptron, not sklearn.linear_model.Perceptron\n",
    "        MultiClassSVM()\n",
    "        #OneVsRestClassifier(PegasosLog())\n",
    "        #OneVsRestClassifier(PegasosHinge())\n",
    "        #OneVsOneClassifier(PegasosLog())\n",
    "        #OneVsOneClassifier(PegasosHinge())\n",
    "        #Pegasos_SVC() \n",
    "        \n",
    "    )\n",
    "\n",
    "    # Train the classifier.\n",
    "    t0 = time.time()\n",
    "    pipeline.fit(Xtrain, Ytrain)\n",
    "    t1 = time.time()\n",
    "    print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "    # Evaluate on the test set.\n",
    "    Yguess = pipeline.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
